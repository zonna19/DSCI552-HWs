{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Hong_Seungil_HW4</h1></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Seungil Hong\n",
    "<br>\n",
    "Github Username: zonna19\n",
    "<br>\n",
    "USC ID: 2375832093 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Time Series Classification Part 1: Feature Creation/Extraction (HW3 Rerun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Obtain Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyexcel_ods\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import neighbors\n",
    "from scipy import stats\n",
    "from builtins import range\n",
    "import os\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the AReM Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data for folder: bending1\n",
      "Using first 2 datasets for testing.\n",
      "Added dataset 1 to test set.\n",
      "Added dataset 2 to test set.\n",
      "Added dataset 3 to training set.\n",
      "Added dataset 4 to training set.\n",
      "Added dataset 5 to training set.\n",
      "Added dataset 6 to training set.\n",
      "Added dataset 7 to training set.\n",
      "\n",
      "Loading data for folder: bending2\n",
      "Using first 2 datasets for testing.\n",
      "Added dataset 1 to test set.\n",
      "Added dataset 2 to test set.\n",
      "Added dataset 3 to training set.\n",
      "Added dataset 4 to training set.\n",
      "Added dataset 5 to training set.\n",
      "Added dataset 6 to training set.\n",
      "\n",
      "Loading data for folder: cycling\n",
      "Using first 3 datasets for testing.\n",
      "Added dataset 1 to test set.\n",
      "Added dataset 2 to test set.\n",
      "Added dataset 3 to test set.\n",
      "Added dataset 4 to training set.\n",
      "Added dataset 5 to training set.\n",
      "Added dataset 6 to training set.\n",
      "Added dataset 7 to training set.\n",
      "Added dataset 8 to training set.\n",
      "Added dataset 9 to training set.\n",
      "Added dataset 10 to training set.\n",
      "Added dataset 11 to training set.\n",
      "Added dataset 12 to training set.\n",
      "Added dataset 13 to training set.\n",
      "Added dataset 14 to training set.\n",
      "Added dataset 15 to training set.\n",
      "Added dataset 16 to training set.\n",
      "\n",
      "Loading data for folder: lying\n",
      "Using first 3 datasets for testing.\n",
      "Added dataset 1 to test set.\n",
      "Added dataset 2 to test set.\n",
      "Added dataset 3 to test set.\n",
      "Added dataset 4 to training set.\n",
      "Added dataset 5 to training set.\n",
      "Added dataset 6 to training set.\n",
      "Added dataset 7 to training set.\n",
      "Added dataset 8 to training set.\n",
      "Added dataset 9 to training set.\n",
      "Added dataset 10 to training set.\n",
      "Added dataset 11 to training set.\n",
      "Added dataset 12 to training set.\n",
      "Added dataset 13 to training set.\n",
      "Added dataset 14 to training set.\n",
      "Added dataset 15 to training set.\n",
      "\n",
      "Loading data for folder: sitting\n",
      "Using first 3 datasets for testing.\n",
      "Added dataset 1 to test set.\n",
      "Added dataset 2 to test set.\n",
      "Added dataset 3 to test set.\n",
      "Added dataset 4 to training set.\n",
      "Added dataset 5 to training set.\n",
      "Added dataset 6 to training set.\n",
      "Added dataset 7 to training set.\n",
      "Added dataset 8 to training set.\n",
      "Added dataset 9 to training set.\n",
      "Added dataset 10 to training set.\n",
      "Added dataset 11 to training set.\n",
      "Added dataset 12 to training set.\n",
      "Added dataset 13 to training set.\n",
      "Added dataset 14 to training set.\n",
      "Added dataset 15 to training set.\n",
      "\n",
      "Loading data for folder: standing\n",
      "Using first 3 datasets for testing.\n",
      "Added dataset 1 to test set.\n",
      "Added dataset 2 to test set.\n",
      "Added dataset 3 to test set.\n",
      "Added dataset 4 to training set.\n",
      "Added dataset 5 to training set.\n",
      "Added dataset 6 to training set.\n",
      "Added dataset 7 to training set.\n",
      "Added dataset 8 to training set.\n",
      "Added dataset 9 to training set.\n",
      "Added dataset 10 to training set.\n",
      "Added dataset 11 to training set.\n",
      "Added dataset 12 to training set.\n",
      "Added dataset 13 to training set.\n",
      "Added dataset 14 to training set.\n",
      "Added dataset 15 to training set.\n",
      "\n",
      "Loading data for folder: walking\n",
      "Using first 3 datasets for testing.\n",
      "Added dataset 1 to test set.\n",
      "Added dataset 2 to test set.\n",
      "Added dataset 3 to test set.\n",
      "Added dataset 4 to training set.\n",
      "Added dataset 5 to training set.\n",
      "Added dataset 6 to training set.\n",
      "Added dataset 7 to training set.\n",
      "Added dataset 8 to training set.\n",
      "Added dataset 9 to training set.\n",
      "Added dataset 10 to training set.\n",
      "Added dataset 11 to training set.\n",
      "Added dataset 12 to training set.\n",
      "Added dataset 13 to training set.\n",
      "Added dataset 14 to training set.\n",
      "Added dataset 15 to training set.\n"
     ]
    }
   ],
   "source": [
    "def list_files_no_hidden(path):\n",
    "    \"\"\"Return a sorted list of all non-hidden files in the directory.\"\"\"\n",
    "    # Ensure that only files are listed (excluding directories)\n",
    "    return sorted([f for f in glob.glob(os.path.join(path, '*')) if os.path.isfile(f) and not os.path.basename(f).startswith('.')])\n",
    "\n",
    "def load_data(file, column_names):\n",
    "    \"\"\"Load data from a CSV file.\"\"\"\n",
    "    try:\n",
    "        # Try reading the data\n",
    "        raw_data = pd.read_csv(file, skiprows=5, header=None) # skip fist 5 rows of general info\n",
    "        \n",
    "        if raw_data.shape[1] != len(column_names):\n",
    "            print(f\"Warning: Mismatch in number of columns for file {file}. Expected {len(column_names)} but got {raw_data.shape[1]}.\")\n",
    "            return None \n",
    "        \n",
    "        raw_data.columns = column_names\n",
    "        return raw_data.replace(np.nan, 0)\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"Error reading file: {file}\")\n",
    "        return None\n",
    "\n",
    "def load_all_datasets(dir_path, folder_name_list, column_name_list, test_set_num):\n",
    "    \"\"\"Load all datasets from the directory.\"\"\"\n",
    "    dataset_dict = dict()\n",
    "    tr_set, te_set = [], []\n",
    "\n",
    "    for folder in folder_name_list:\n",
    "        dataset_list = []\n",
    "        path = os.path.join(dir_path, folder)\n",
    "        files = list_files_no_hidden(path)\n",
    "        # Sort files\n",
    "        files_sorted = sorted(files, key=lambda x: int(re.search(r'\\D*(\\d+)', x).group(1)))\n",
    "        set_num = test_set_num.get(folder, 3)\n",
    "        print(f\"\\nLoading data for folder: {folder}\")\n",
    "        print(f\"Using first {set_num} datasets for testing.\")\n",
    "\n",
    "        for idx, file in enumerate(files_sorted):\n",
    "            data = load_data(file, column_name_list)\n",
    "            dataset_list.append(data)\n",
    "            if idx < set_num:\n",
    "                te_set.append(data)\n",
    "                print(f\"Added dataset {idx+1} to test set.\")\n",
    "            else:\n",
    "                tr_set.append(data)\n",
    "                print(f\"Added dataset {idx+1} to training set.\")\n",
    "\n",
    "        dataset_dict[folder] = dataset_list\n",
    "\n",
    "    return dataset_dict, tr_set, te_set\n",
    "\n",
    "# Constants\n",
    "DIR_PATH = '../data/AReM/'\n",
    "FOLDER_LIST = ['bending1', 'bending2', 'cycling', 'lying', 'sitting', 'standing', 'walking']\n",
    "COLUMN_LIST = ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
    "# Specify how many test sets there are per folder\n",
    "TEST_SET_NUM = {'bending1': 2, 'bending2': 2}\n",
    "\n",
    "dataset_dict, tr_set, te_set = load_all_datasets(DIR_PATH, FOLDER_LIST, COLUMN_LIST, TEST_SET_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_no_hidden(path):\n",
    "    \"\"\"Return a sorted list of all non-hidden files in the directory.\"\"\"\n",
    "    # Ensure that only files are listed (excluding directories)\n",
    "    return sorted([f for f in glob.glob(os.path.join(path, '*')) if os.path.isfile(f) and not os.path.basename(f).startswith('.')])\n",
    "\n",
    "def load_data(file, column_names):\n",
    "    \"\"\"Load data from a CSV file.\"\"\"\n",
    "    try:\n",
    "        # Try reading the data\n",
    "        raw_data = pd.read_csv(file, skiprows=5, header=None) # skip fist 5 rows of general info\n",
    "        \n",
    "        if raw_data.shape[1] != len(column_names):\n",
    "            print(f\"Warning: Mismatch in number of columns for file {file}. Expected {len(column_names)} but got {raw_data.shape[1]}.\")\n",
    "            return None \n",
    "        \n",
    "        raw_data.columns = column_names\n",
    "        return raw_data.replace(np.nan, 0)\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"Error reading file: {file}\")\n",
    "        return None\n",
    "\n",
    "def load_all_datasets(dir_path, folder_name_list, column_name_list, test_set_num):\n",
    "    \"\"\"Load all datasets from the directory.\"\"\"\n",
    "    dataset_dict = dict()\n",
    "    tr_set, te_set = [], []\n",
    "\n",
    "    for folder in folder_name_list:\n",
    "        dataset_list = []\n",
    "        path = os.path.join(dir_path, folder)\n",
    "        files = list_files_no_hidden(path)\n",
    "        # Sort files\n",
    "        files_sorted = sorted(files, key=lambda x: int(re.search(r'\\D*(\\d+)', x).group(1)))\n",
    "        set_num = test_set_num.get(folder, 3)\n",
    "        print(f\"\\nLoading data for folder: {folder}\")\n",
    "        print(f\"Using first {set_num} datasets for testing.\")\n",
    "\n",
    "        for idx, file in enumerate(files_sorted):\n",
    "            data = load_data(file, column_name_list)\n",
    "            dataset_list.append(data)\n",
    "            if idx < set_num:\n",
    "                te_set.append(data)\n",
    "                print(f\"Added dataset {idx+1} to test set.\")\n",
    "            else:\n",
    "                tr_set.append(data)\n",
    "                print(f\"Added dataset {idx+1} to training set.\")\n",
    "\n",
    "        dataset_dict[folder] = dataset_list\n",
    "\n",
    "    return dataset_dict, tr_set, te_set\n",
    "\n",
    "# Constants\n",
    "DIR_PATH = '../data/AReM/'\n",
    "FOLDER_LIST = ['bending1', 'bending2', 'cycling', 'lying', 'sitting', 'standing', 'walking']\n",
    "COLUMN_LIST = ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
    "# Specify how many test sets there are per folder\n",
    "TEST_SET_NUM = {'bending1': 2, 'bending2': 2}\n",
    "\n",
    "dataset_dict, tr_set, te_set = load_all_datasets(DIR_PATH, FOLDER_LIST, COLUMN_LIST, TEST_SET_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. Research \n",
    "#### Research what types of time-domain features are usually used in time series classification and list them (examples are minimum, maximum, mean, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mean\n",
    "* Median\n",
    "* Variance, Standard Deviation\n",
    "* Skewness: A measure of the asymmetry of the data distribution.\n",
    "* Kurtosis: A measure of the tailedness of the data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     min1   max1      mean1  median1      std1  1st Quartile1  3rd Quartile1  \\\n",
      "1   37.25  45.00  40.624792    40.50  1.476967          39.25        42.0000   \n",
      "2   38.00  45.67  42.812812    42.50  1.435550          42.00        43.6700   \n",
      "3   35.00  47.40  43.954500    44.33  1.558835          43.00        45.0000   \n",
      "4   33.00  47.75  42.179812    43.50  3.670666          39.15        45.0000   \n",
      "5   33.00  45.75  41.678063    41.75  2.243490          41.33        42.7500   \n",
      "..    ...    ...        ...      ...       ...            ...            ...   \n",
      "85  19.50  45.33  33.586875    34.25  4.650935          30.25        37.0000   \n",
      "86  19.75  45.50  34.322750    35.25  4.752477          31.00        38.0000   \n",
      "87  19.50  46.00  34.546229    35.25  4.842294          31.25        37.8125   \n",
      "88  23.50  46.25  34.873229    35.25  4.531720          31.75        38.2500   \n",
      "89  19.25  44.00  34.473188    35.00  4.796705          31.25        38.0000   \n",
      "\n",
      "    min2   max2     mean2  ...      std5  1st Quartile5  3rd Quartile5  min6  \\\n",
      "1    0.0   1.30  0.358604  ...  2.188449        33.0000          36.00  0.00   \n",
      "2    0.0   1.22  0.372437  ...  1.995255        32.0000          34.50  0.00   \n",
      "3    0.0   1.70  0.426250  ...  1.999604        35.3625          36.50  0.00   \n",
      "4    0.0   3.00  0.696042  ...  3.849448        30.4575          36.33  0.00   \n",
      "5    0.0   2.83  0.535979  ...  2.411026        28.4575          31.25  0.00   \n",
      "..   ...    ...       ...  ...       ...            ...            ...   ...   \n",
      "85   0.0  14.67  4.576562  ...  3.283983        13.7300          18.25  0.00   \n",
      "86   0.0  13.47  4.456333  ...  3.119856        13.5000          17.75  0.00   \n",
      "87   0.0  12.47  4.371958  ...  2.823124        14.0000          17.75  0.00   \n",
      "88   0.0  14.82  4.380583  ...  3.131076        13.7500          18.00  0.00   \n",
      "89   0.0  13.86  4.359312  ...  3.156320        13.7300          17.75  0.43   \n",
      "\n",
      "     max6     mean6  median6      std6  1st Quartile6  3rd Quartile6  \n",
      "1    1.92  0.570583     0.43  0.582915         0.0000         1.3000  \n",
      "2    3.11  0.571083     0.43  0.601010         0.0000         1.3000  \n",
      "3    1.79  0.493292     0.43  0.513506         0.0000         0.9400  \n",
      "4    2.18  0.613521     0.50  0.524317         0.0000         1.0000  \n",
      "5    1.79  0.383292     0.43  0.389164         0.0000         0.5000  \n",
      "..    ...       ...      ...       ...            ...            ...  \n",
      "85   8.32  3.259729     3.11  1.640243         2.0500         4.3225  \n",
      "86   9.67  3.432563     3.20  1.732727         2.1575         4.5650  \n",
      "87  10.00  3.338125     3.08  1.656742         2.1600         4.3350  \n",
      "88   9.51  3.424646     3.27  1.690960         2.1700         4.5000  \n",
      "89   9.00  3.340458     3.09  1.699114         2.1200         4.3750  \n",
      "\n",
      "[89 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "# Function to extract features for a single column\n",
    "def extract_features_for_column(series):\n",
    "    return {\n",
    "        'min': series.min(),\n",
    "        'max': series.max(),\n",
    "        'mean': series.mean(),\n",
    "        'median': series.median(),\n",
    "        'std': series.std(),\n",
    "        '1st Quartile': series.quantile(0.25),\n",
    "        '3rd Quartile': series.quantile(0.75)\n",
    "    }\n",
    "\n",
    "# Extract features for each dataset and store in a list\n",
    "features_list = []\n",
    "\n",
    "for folder in FOLDER_LIST:\n",
    "    for dataset in dataset_dict[folder]:\n",
    "        \n",
    "        dataset_features = {}\n",
    "        for idx, col in enumerate(COLUMN_LIST[1:], start=1):\n",
    "            col_features = extract_features_for_column(dataset[col])\n",
    "            for key, value in col_features.items():\n",
    "                dataset_features[f\"{key}{idx}\"] = value\n",
    "        features_list.append(dataset_features)\n",
    "\n",
    "# Convert the list of feature dictionaries to a pandas DataFrame\n",
    "instance_features = pd.DataFrame(features_list)\n",
    "instance_features.index += 1 # start from 1\n",
    "\n",
    "print(instance_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. Estimate the standard deviation of each of the time-domain features you extracted from the data. Then, use Pythonâ€™s bootstrapped or any other method to build a 90% bootsrap confidence interval for the standard deviation of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Observed Std</th>\n",
       "      <th>Bootstrap Std</th>\n",
       "      <th>Lower 90% CI</th>\n",
       "      <th>Upper 90% CI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>min1</th>\n",
       "      <td>9.466</td>\n",
       "      <td>9.370</td>\n",
       "      <td>8.099</td>\n",
       "      <td>10.674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max1</th>\n",
       "      <td>4.347</td>\n",
       "      <td>4.293</td>\n",
       "      <td>3.263</td>\n",
       "      <td>5.276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean1</th>\n",
       "      <td>5.279</td>\n",
       "      <td>5.245</td>\n",
       "      <td>4.692</td>\n",
       "      <td>5.822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median1</th>\n",
       "      <td>5.385</td>\n",
       "      <td>5.316</td>\n",
       "      <td>4.728</td>\n",
       "      <td>5.903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std1</th>\n",
       "      <td>1.753</td>\n",
       "      <td>1.734</td>\n",
       "      <td>1.552</td>\n",
       "      <td>1.930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st Quartile1</th>\n",
       "      <td>6.090</td>\n",
       "      <td>6.052</td>\n",
       "      <td>5.528</td>\n",
       "      <td>6.578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3rd Quartile1</th>\n",
       "      <td>5.082</td>\n",
       "      <td>5.019</td>\n",
       "      <td>4.322</td>\n",
       "      <td>5.737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max2</th>\n",
       "      <td>5.012</td>\n",
       "      <td>4.980</td>\n",
       "      <td>4.603</td>\n",
       "      <td>5.381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean2</th>\n",
       "      <td>1.560</td>\n",
       "      <td>1.546</td>\n",
       "      <td>1.384</td>\n",
       "      <td>1.686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median2</th>\n",
       "      <td>1.399</td>\n",
       "      <td>1.394</td>\n",
       "      <td>1.244</td>\n",
       "      <td>1.527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std2</th>\n",
       "      <td>0.878</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st Quartile2</th>\n",
       "      <td>0.937</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.819</td>\n",
       "      <td>1.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3rd Quartile2</th>\n",
       "      <td>2.106</td>\n",
       "      <td>2.090</td>\n",
       "      <td>1.887</td>\n",
       "      <td>2.277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min3</th>\n",
       "      <td>2.952</td>\n",
       "      <td>2.933</td>\n",
       "      <td>2.744</td>\n",
       "      <td>3.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max3</th>\n",
       "      <td>4.836</td>\n",
       "      <td>4.791</td>\n",
       "      <td>4.172</td>\n",
       "      <td>5.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean3</th>\n",
       "      <td>3.965</td>\n",
       "      <td>3.921</td>\n",
       "      <td>3.392</td>\n",
       "      <td>4.444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median3</th>\n",
       "      <td>3.991</td>\n",
       "      <td>3.959</td>\n",
       "      <td>3.387</td>\n",
       "      <td>4.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std3</th>\n",
       "      <td>0.938</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.755</td>\n",
       "      <td>1.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st Quartile3</th>\n",
       "      <td>4.175</td>\n",
       "      <td>4.114</td>\n",
       "      <td>3.562</td>\n",
       "      <td>4.665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3rd Quartile3</th>\n",
       "      <td>4.125</td>\n",
       "      <td>4.067</td>\n",
       "      <td>3.460</td>\n",
       "      <td>4.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max4</th>\n",
       "      <td>2.163</td>\n",
       "      <td>2.148</td>\n",
       "      <td>1.952</td>\n",
       "      <td>2.337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean4</th>\n",
       "      <td>1.161</td>\n",
       "      <td>1.154</td>\n",
       "      <td>1.074</td>\n",
       "      <td>1.214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median4</th>\n",
       "      <td>1.140</td>\n",
       "      <td>1.131</td>\n",
       "      <td>1.053</td>\n",
       "      <td>1.190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std4</th>\n",
       "      <td>0.456</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st Quartile4</th>\n",
       "      <td>0.839</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3rd Quartile4</th>\n",
       "      <td>1.546</td>\n",
       "      <td>1.533</td>\n",
       "      <td>1.425</td>\n",
       "      <td>1.618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min5</th>\n",
       "      <td>6.061</td>\n",
       "      <td>5.958</td>\n",
       "      <td>4.301</td>\n",
       "      <td>7.416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max5</th>\n",
       "      <td>5.681</td>\n",
       "      <td>5.650</td>\n",
       "      <td>4.704</td>\n",
       "      <td>6.541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean5</th>\n",
       "      <td>5.611</td>\n",
       "      <td>5.537</td>\n",
       "      <td>4.410</td>\n",
       "      <td>6.705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median5</th>\n",
       "      <td>5.748</td>\n",
       "      <td>5.624</td>\n",
       "      <td>4.378</td>\n",
       "      <td>6.801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std5</th>\n",
       "      <td>1.016</td>\n",
       "      <td>1.005</td>\n",
       "      <td>0.818</td>\n",
       "      <td>1.205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st Quartile5</th>\n",
       "      <td>6.028</td>\n",
       "      <td>5.961</td>\n",
       "      <td>4.839</td>\n",
       "      <td>7.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3rd Quartile5</th>\n",
       "      <td>5.469</td>\n",
       "      <td>5.417</td>\n",
       "      <td>4.322</td>\n",
       "      <td>6.539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min6</th>\n",
       "      <td>0.045</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max6</th>\n",
       "      <td>2.498</td>\n",
       "      <td>2.484</td>\n",
       "      <td>2.233</td>\n",
       "      <td>2.748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean6</th>\n",
       "      <td>1.150</td>\n",
       "      <td>1.142</td>\n",
       "      <td>1.057</td>\n",
       "      <td>1.204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median6</th>\n",
       "      <td>1.080</td>\n",
       "      <td>1.073</td>\n",
       "      <td>0.989</td>\n",
       "      <td>1.138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std6</th>\n",
       "      <td>0.515</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st Quartile6</th>\n",
       "      <td>0.754</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3rd Quartile6</th>\n",
       "      <td>1.519</td>\n",
       "      <td>1.510</td>\n",
       "      <td>1.408</td>\n",
       "      <td>1.595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Observed Std  Bootstrap Std  Lower 90% CI  Upper 90% CI\n",
       "Feature                                                               \n",
       "min1                  9.466          9.370         8.099        10.674\n",
       "max1                  4.347          4.293         3.263         5.276\n",
       "mean1                 5.279          5.245         4.692         5.822\n",
       "median1               5.385          5.316         4.728         5.903\n",
       "std1                  1.753          1.734         1.552         1.930\n",
       "1st Quartile1         6.090          6.052         5.528         6.578\n",
       "3rd Quartile1         5.082          5.019         4.322         5.737\n",
       "min2                  0.000          0.000         0.000         0.000\n",
       "max2                  5.012          4.980         4.603         5.381\n",
       "mean2                 1.560          1.546         1.384         1.686\n",
       "median2               1.399          1.394         1.244         1.527\n",
       "std2                  0.878          0.870         0.804         0.929\n",
       "1st Quartile2         0.937          0.928         0.819         1.022\n",
       "3rd Quartile2         2.106          2.090         1.887         2.277\n",
       "min3                  2.952          2.933         2.744         3.095\n",
       "max3                  4.836          4.791         4.172         5.385\n",
       "mean3                 3.965          3.921         3.392         4.444\n",
       "median3               3.991          3.959         3.387         4.488\n",
       "std3                  0.938          0.927         0.755         1.120\n",
       "1st Quartile3         4.175          4.114         3.562         4.665\n",
       "3rd Quartile3         4.125          4.067         3.460         4.600\n",
       "min4                  0.000          0.000         0.000         0.000\n",
       "max4                  2.163          2.148         1.952         2.337\n",
       "mean4                 1.161          1.154         1.074         1.214\n",
       "median4               1.140          1.131         1.053         1.190\n",
       "std4                  0.456          0.452         0.419         0.482\n",
       "1st Quartile4         0.839          0.833         0.772         0.884\n",
       "3rd Quartile4         1.546          1.533         1.425         1.618\n",
       "min5                  6.061          5.958         4.301         7.416\n",
       "max5                  5.681          5.650         4.704         6.541\n",
       "mean5                 5.611          5.537         4.410         6.705\n",
       "median5               5.748          5.624         4.378         6.801\n",
       "std5                  1.016          1.005         0.818         1.205\n",
       "1st Quartile5         6.028          5.961         4.839         7.091\n",
       "3rd Quartile5         5.469          5.417         4.322         6.539\n",
       "min6                  0.045          0.036         0.000         0.078\n",
       "max6                  2.498          2.484         2.233         2.748\n",
       "mean6                 1.150          1.142         1.057         1.204\n",
       "median6               1.080          1.073         0.989         1.138\n",
       "std6                  0.515          0.512         0.478         0.544\n",
       "1st Quartile6         0.754          0.749         0.686         0.801\n",
       "3rd Quartile6         1.519          1.510         1.408         1.595"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Number of bootstrap samples usually 1000times widely used.\n",
    "N_BST = 1000 \n",
    "\n",
    "# Function to generate a bootstrap sample\n",
    "def bst_sample(data):\n",
    "    return np.random.choice(data, size=len(data), replace=True)  # replace of data is key for bootstrap method\n",
    "\n",
    "# Function to compute bootstrap confidence interval for standard deviation\n",
    "def bst_std_ci(data, alpha=0.10): # 90% C.I\n",
    "    observed_std = np.std(data)\n",
    "    bst_stds = []\n",
    "\n",
    "    for _ in range(N_BST):\n",
    "        sample = bst_sample(data)\n",
    "        bst_stds.append(np.std(sample))\n",
    "    \n",
    "    lower = np.percentile(bst_stds, 100 * alpha / 2)\n",
    "    upper = np.percentile(bst_stds, 100 * (1 - alpha / 2))\n",
    "    \n",
    "    mean_bst_std = np.mean(bst_stds)\n",
    "    \n",
    "    return observed_std, mean_bst_std, lower, upper\n",
    "\n",
    "# Extract the standard deviation and confidence interval for each feature\n",
    "bst_results = {\n",
    "    'Feature': [],\n",
    "    'Observed Std': [],\n",
    "    'Bootstrap Std': [],\n",
    "    'Lower 90% CI': [],\n",
    "    'Upper 90% CI': []\n",
    "}\n",
    "\n",
    "for col in instance_features.columns:\n",
    "    observed_std, mean_bst_std, lower, upper = bst_std_ci(instance_features[col])\n",
    "    \n",
    "    bst_results['Feature'].append(col)\n",
    "    bst_results['Observed Std'].append(observed_std)\n",
    "    bst_results['Bootstrap Std'].append(mean_bst_std)\n",
    "    bst_results['Lower 90% CI'].append(lower)\n",
    "    bst_results['Upper 90% CI'].append(upper)\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "df_bst = pd.DataFrame(bst_results).round(3)\n",
    "df_bst.set_index('Feature', inplace=True)\n",
    "df_bst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv. Use your judgement to select the three most important time-domain features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mean : is the key feature to tell the data\n",
    "* Standard Deviation : Along with the mean, std tells dataset is how much are they spread. Also tells some indication of noises.\n",
    "* Median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Time Series Classification Part 2: Binary and Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Binary Classification Using Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### i. Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ii. Splitted Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### iii. Time Series Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### iv. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### v. Test Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### vi. Separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### vii. Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Binary Classification Using L1-penalized logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### i. Time Series Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ii. Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Multi-class Classification (The Realistic Case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### i. Time Series Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ii. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ii. Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ISLR 4.8.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ISLR 4.8.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Extra Practice ISLR 4.8.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Extra Practice ISLR 4.8.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "294.435px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "3c20c2d94d2527936fe0f3a300eb11db30fed84423423838e2f93b74eb7aaebc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
